{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP with Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform sentiment analysis on tweet dataset using Logistic Regression<br>\n",
    "The steps ionvolved in this are<br>\n",
    "1. preprocessing (tokenization, removing stopwords, stemming)\n",
    "2. Frequency count of each word in the each category i.e. positive and negative\n",
    "3. vectorization of the words\n",
    "4. training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords, twitter_samples\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = positive_tweets + negative_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.append(np.ones((len(positive_tweets), 1)), np.zeros((len(negative_tweets), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My beautiful sunflowers on a sunny Friday morning off :) #sunflowers #favourites #happy #Friday off… https://t.co/3tfYom0N1i'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweets[2277]\n",
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My beautiful sunflowers on a sunny Friday morning off :) sunflowers favourites happy Friday off… '"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "tweet = re.sub(r'#', '', tweet)\n",
    "tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['my',\n",
       " 'beautiful',\n",
       " 'sunflowers',\n",
       " 'on',\n",
       " 'a',\n",
       " 'sunny',\n",
       " 'friday',\n",
       " 'morning',\n",
       " 'off',\n",
       " ':)',\n",
       " 'sunflowers',\n",
       " 'favourites',\n",
       " 'happy',\n",
       " 'friday',\n",
       " 'off',\n",
       " '…']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "tweet_tokens = tokenizer.tokenize(tweet)\n",
    "tweet_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beautiful',\n",
       " 'sunflowers',\n",
       " 'sunny',\n",
       " 'friday',\n",
       " 'morning',\n",
       " ':)',\n",
       " 'sunflowers',\n",
       " 'favourites',\n",
       " 'happy',\n",
       " 'friday',\n",
       " '…']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords_english = stopwords.words('english')\n",
    "clean_tweet = []\n",
    "\n",
    "for word in tweet_tokens:\n",
    "    if(word not in stopwords_english and word not in string.punctuation):\n",
    "        clean_tweet.append(word)\n",
    "\n",
    "clean_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beauti',\n",
       " 'sunflow',\n",
       " 'sunni',\n",
       " 'friday',\n",
       " 'morn',\n",
       " ':)',\n",
       " 'sunflow',\n",
       " 'favourit',\n",
       " 'happi',\n",
       " 'friday',\n",
       " '…']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stem_tweet = []\n",
    "\n",
    "for word in clean_tweet:\n",
    "    stem_word = stemmer.stem(word)\n",
    "    stem_tweet.append(stem_word)\n",
    "\n",
    "stem_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> now we will create a function which will perform all these steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    clean_tweet = []\n",
    "\n",
    "    for word in tweet_tokens:\n",
    "        if(word not in stopwords_english and word not in string.punctuation):\n",
    "            clean_tweet.append(word)\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    stem_tweet = []\n",
    "\n",
    "    for word in clean_tweet:\n",
    "        stem_word = stemmer.stem(word)\n",
    "        stem_tweet.append(stem_word)\n",
    "\n",
    "    return stem_tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering and Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beauti',\n",
       " 'sunflow',\n",
       " 'sunni',\n",
       " 'friday',\n",
       " 'morn',\n",
       " ':)',\n",
       " 'sunflow',\n",
       " 'favourit',\n",
       " 'happi',\n",
       " 'friday',\n",
       " '…']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweets[2277]\n",
    "after_preprocessing = process_tweet(tweet)\n",
    "after_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_label = labels[2277]\n",
    "tweet_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('beauti', 1): 1,\n",
       " ('sunflow', 1): 2,\n",
       " ('sunni', 1): 1,\n",
       " ('friday', 1): 2,\n",
       " ('morn', 1): 1,\n",
       " (':)', 1): 1,\n",
       " ('favourit', 1): 1,\n",
       " ('happi', 1): 1,\n",
       " ('…', 1): 1}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs = {}\n",
    "for word in after_preprocessing:\n",
    "    pair = (word, 1)\n",
    "    if(pair in freqs.keys()):\n",
    "        freqs[pair] += 1\n",
    "    else:\n",
    "        freqs[pair] = 1\n",
    "\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we will calculate frequency of each word in each label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets,labels):\n",
    "    freqs = {}\n",
    "    label_list = np.squeeze(labels).tolist()\n",
    "    for label, tweet in zip(label_list, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, label)\n",
    "            if(pair in freqs.keys()):\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = build_freqs(tweets, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('followfriday', 1.0): 25,\n",
       " ('top', 1.0): 32,\n",
       " ('engag', 1.0): 7,\n",
       " ('member', 1.0): 16,\n",
       " ('commun', 1.0): 33,\n",
       " ('week', 1.0): 83,\n",
       " (':)', 1.0): 3568,\n",
       " ('hey', 1.0): 76,\n",
       " ('jame', 1.0): 7,\n",
       " ('odd', 1.0): 2,\n",
       " (':/', 1.0): 5,\n",
       " ('pleas', 1.0): 97,\n",
       " ('call', 1.0): 37,\n",
       " ('contact', 1.0): 7,\n",
       " ('centr', 1.0): 2,\n",
       " ('02392441234', 1.0): 1,\n",
       " ('abl', 1.0): 8,\n",
       " ('assist', 1.0): 1,\n",
       " ('mani', 1.0): 33,\n",
       " ('thank', 1.0): 620,\n",
       " ('listen', 1.0): 16,\n",
       " ('last', 1.0): 47,\n",
       " ('night', 1.0): 68,\n",
       " ('bleed', 1.0): 2,\n",
       " ('amaz', 1.0): 51,\n",
       " ('track', 1.0): 5,\n",
       " ('scotland', 1.0): 2,\n",
       " ('congrat', 1.0): 21,\n",
       " ('yeaaah', 1.0): 1,\n",
       " ('yipppi', 1.0): 1,\n",
       " ('accnt', 1.0): 2,\n",
       " ('verifi', 1.0): 2,\n",
       " ('rqst', 1.0): 1,\n",
       " ('succeed', 1.0): 1,\n",
       " ('got', 1.0): 69,\n",
       " ('blue', 1.0): 9,\n",
       " ('tick', 1.0): 1,\n",
       " ('mark', 1.0): 1,\n",
       " ('fb', 1.0): 6,\n",
       " ('profil', 1.0): 2,\n",
       " ('15', 1.0): 5,\n",
       " ('day', 1.0): 246,\n",
       " ('one', 1.0): 129,\n",
       " ('irresist', 1.0): 2,\n",
       " ('flipkartfashionfriday', 1.0): 17,\n",
       " ('like', 1.0): 233,\n",
       " ('keep', 1.0): 68,\n",
       " ('love', 1.0): 400,\n",
       " ('custom', 1.0): 4,\n",
       " ('wait', 1.0): 70,\n",
       " ('long', 1.0): 36,\n",
       " ('hope', 1.0): 141,\n",
       " ('enjoy', 1.0): 75,\n",
       " ('happi', 1.0): 211,\n",
       " ('friday', 1.0): 116,\n",
       " ('lwwf', 1.0): 1,\n",
       " ('second', 1.0): 10,\n",
       " ('thought', 1.0): 29,\n",
       " ('’', 1.0): 21,\n",
       " ('enough', 1.0): 18,\n",
       " ('time', 1.0): 127,\n",
       " ('dd', 1.0): 1,\n",
       " ('new', 1.0): 143,\n",
       " ('short', 1.0): 7,\n",
       " ('enter', 1.0): 9,\n",
       " ('system', 1.0): 2,\n",
       " ('sheep', 1.0): 1,\n",
       " ('must', 1.0): 18,\n",
       " ('buy', 1.0): 11,\n",
       " ('jgh', 1.0): 4,\n",
       " ('go', 1.0): 148,\n",
       " ('bayan', 1.0): 1,\n",
       " (':d', 1.0): 629,\n",
       " ('bye', 1.0): 7,\n",
       " ('act', 1.0): 8,\n",
       " ('mischiev', 1.0): 1,\n",
       " ('etl', 1.0): 1,\n",
       " ('layer', 1.0): 1,\n",
       " ('in-hous', 1.0): 1,\n",
       " ('wareh', 1.0): 1,\n",
       " ('app', 1.0): 16,\n",
       " ('katamari', 1.0): 1,\n",
       " ('well', 1.0): 81,\n",
       " ('…', 1.0): 38,\n",
       " ('name', 1.0): 18,\n",
       " ('impli', 1.0): 1,\n",
       " (':p', 1.0): 138,\n",
       " ('influenc', 1.0): 18,\n",
       " ('big', 1.0): 33,\n",
       " ('...', 1.0): 289,\n",
       " ('juici', 1.0): 3,\n",
       " ('selfi', 1.0): 12,\n",
       " ('follow', 1.0): 381,\n",
       " ('perfect', 1.0): 24,\n",
       " ('alreadi', 1.0): 28,\n",
       " ('know', 1.0): 145,\n",
       " (\"what'\", 1.0): 17,\n",
       " ('great', 1.0): 171,\n",
       " ('opportun', 1.0): 23,\n",
       " ('junior', 1.0): 2,\n",
       " ('triathlet', 1.0): 1,\n",
       " ('age', 1.0): 2,\n",
       " ('12', 1.0): 5,\n",
       " ('13', 1.0): 6,\n",
       " ('gatorad', 1.0): 1,\n",
       " ('seri', 1.0): 5,\n",
       " ('get', 1.0): 206,\n",
       " ('entri', 1.0): 4,\n",
       " ('lay', 1.0): 4,\n",
       " ('greet', 1.0): 5,\n",
       " ('card', 1.0): 8,\n",
       " ('rang', 1.0): 3,\n",
       " ('print', 1.0): 3,\n",
       " ('today', 1.0): 108,\n",
       " ('job', 1.0): 41,\n",
       " (':-)', 1.0): 692,\n",
       " (\"friend'\", 1.0): 3,\n",
       " ('lunch', 1.0): 5,\n",
       " ('yummm', 1.0): 1,\n",
       " ('nostalgia', 1.0): 1,\n",
       " ('tb', 1.0): 2,\n",
       " ('ku', 1.0): 1,\n",
       " ('id', 1.0): 8,\n",
       " ('conflict', 1.0): 1,\n",
       " ('help', 1.0): 41,\n",
       " (\"here'\", 1.0): 25,\n",
       " ('screenshot', 1.0): 3,\n",
       " ('work', 1.0): 110,\n",
       " ('hi', 1.0): 173,\n",
       " ('liv', 1.0): 2,\n",
       " ('hello', 1.0): 59,\n",
       " ('need', 1.0): 78,\n",
       " ('someth', 1.0): 28,\n",
       " ('u', 1.0): 175,\n",
       " ('fm', 1.0): 2,\n",
       " ('twitter', 1.0): 29,\n",
       " ('—', 1.0): 27,\n",
       " ('sure', 1.0): 58,\n",
       " ('thing', 1.0): 69,\n",
       " ('dm', 1.0): 39,\n",
       " ('x', 1.0): 72,\n",
       " (\"i'v\", 1.0): 35,\n",
       " ('heard', 1.0): 9,\n",
       " ('four', 1.0): 5,\n",
       " ('season', 1.0): 9,\n",
       " ('pretti', 1.0): 20,\n",
       " ('dope', 1.0): 2,\n",
       " ('penthous', 1.0): 1,\n",
       " ('obv', 1.0): 1,\n",
       " ('gobigorgohom', 1.0): 1,\n",
       " ('fun', 1.0): 58,\n",
       " (\"y'all\", 1.0): 3,\n",
       " ('yeah', 1.0): 47,\n",
       " ('suppos', 1.0): 7,\n",
       " ('lol', 1.0): 64,\n",
       " ('chat', 1.0): 13,\n",
       " ('bit', 1.0): 20,\n",
       " ('youth', 1.0): 19,\n",
       " ('💅🏽', 1.0): 1,\n",
       " ('💋', 1.0): 2,\n",
       " ('seen', 1.0): 10,\n",
       " ('year', 1.0): 43,\n",
       " ('rest', 1.0): 12,\n",
       " ('goe', 1.0): 7,\n",
       " ('quickli', 1.0): 3,\n",
       " ('bed', 1.0): 16,\n",
       " ('music', 1.0): 21,\n",
       " ('fix', 1.0): 10,\n",
       " ('dream', 1.0): 20,\n",
       " ('spiritu', 1.0): 1,\n",
       " ('ritual', 1.0): 1,\n",
       " ('festiv', 1.0): 8,\n",
       " ('népal', 1.0): 1,\n",
       " ('begin', 1.0): 4,\n",
       " ('line-up', 1.0): 4,\n",
       " ('left', 1.0): 13,\n",
       " ('see', 1.0): 184,\n",
       " ('sarah', 1.0): 4,\n",
       " ('send', 1.0): 22,\n",
       " ('us', 1.0): 109,\n",
       " ('email', 1.0): 26,\n",
       " ('bitsy@bitdefender.com', 1.0): 1,\n",
       " (\"we'll\", 1.0): 20,\n",
       " ('asap', 1.0): 5,\n",
       " ('kik', 1.0): 22,\n",
       " ('hatessuc', 1.0): 1,\n",
       " ('32429', 1.0): 1,\n",
       " ('kikm', 1.0): 1,\n",
       " ('lgbt', 1.0): 2,\n",
       " ('tinder', 1.0): 1,\n",
       " ('nsfw', 1.0): 1,\n",
       " ('akua', 1.0): 1,\n",
       " ('cumshot', 1.0): 1,\n",
       " ('come', 1.0): 70,\n",
       " ('hous', 1.0): 7,\n",
       " ('nsn_supplement', 1.0): 1,\n",
       " ('effect', 1.0): 4,\n",
       " ('press', 1.0): 1,\n",
       " ('releas', 1.0): 11,\n",
       " ('distribut', 1.0): 1,\n",
       " ('result', 1.0): 2,\n",
       " ('link', 1.0): 18,\n",
       " ('remov', 1.0): 3,\n",
       " ('pressreleas', 1.0): 1,\n",
       " ('newsdistribut', 1.0): 1,\n",
       " ('bam', 1.0): 44,\n",
       " ('bestfriend', 1.0): 50,\n",
       " ('lot', 1.0): 87,\n",
       " ('warsaw', 1.0): 44,\n",
       " ('<3', 1.0): 134,\n",
       " ('x46', 1.0): 1,\n",
       " ('everyon', 1.0): 58,\n",
       " ('watch', 1.0): 46,\n",
       " ('documentari', 1.0): 1,\n",
       " ('earthl', 1.0): 2,\n",
       " ('youtub', 1.0): 13,\n",
       " ('support', 1.0): 27,\n",
       " ('buuut', 1.0): 1,\n",
       " ('oh', 1.0): 53,\n",
       " ('look', 1.0): 137,\n",
       " ('forward', 1.0): 29,\n",
       " ('visit', 1.0): 30,\n",
       " ('next', 1.0): 48,\n",
       " ('letsgetmessi', 1.0): 1,\n",
       " ('jo', 1.0): 1,\n",
       " ('make', 1.0): 99,\n",
       " ('feel', 1.0): 46,\n",
       " ('better', 1.0): 52,\n",
       " ('never', 1.0): 36,\n",
       " ('anyon', 1.0): 11,\n",
       " ('kpop', 1.0): 1,\n",
       " ('flesh', 1.0): 1,\n",
       " ('good', 1.0): 238,\n",
       " ('girl', 1.0): 44,\n",
       " ('best', 1.0): 65,\n",
       " ('wish', 1.0): 37,\n",
       " ('reason', 1.0): 13,\n",
       " ('epic', 1.0): 2,\n",
       " ('soundtrack', 1.0): 1,\n",
       " ('shout', 1.0): 12,\n",
       " ('ad', 1.0): 14,\n",
       " ('video', 1.0): 34,\n",
       " ('playlist', 1.0): 5,\n",
       " ('would', 1.0): 84,\n",
       " ('dear', 1.0): 17,\n",
       " ('jordan', 1.0): 1,\n",
       " ('okay', 1.0): 39,\n",
       " ('fake', 1.0): 2,\n",
       " ('gameplay', 1.0): 2,\n",
       " (';)', 1.0): 27,\n",
       " ('haha', 1.0): 53,\n",
       " ('im', 1.0): 51,\n",
       " ('kid', 1.0): 18,\n",
       " ('stuff', 1.0): 13,\n",
       " ('exactli', 1.0): 6,\n",
       " ('product', 1.0): 12,\n",
       " ('line', 1.0): 6,\n",
       " ('etsi', 1.0): 1,\n",
       " ('shop', 1.0): 16,\n",
       " ('check', 1.0): 52,\n",
       " ('vacat', 1.0): 6,\n",
       " ('recharg', 1.0): 1,\n",
       " ('normal', 1.0): 6,\n",
       " ('charger', 1.0): 2,\n",
       " ('asleep', 1.0): 9,\n",
       " ('talk', 1.0): 45,\n",
       " ('sooo', 1.0): 6,\n",
       " ('someon', 1.0): 34,\n",
       " ('text', 1.0): 18,\n",
       " ('ye', 1.0): 77,\n",
       " ('bet', 1.0): 6,\n",
       " (\"he'll\", 1.0): 4,\n",
       " ('fit', 1.0): 3,\n",
       " ('hear', 1.0): 33,\n",
       " ('speech', 1.0): 1,\n",
       " ('piti', 1.0): 3,\n",
       " ('green', 1.0): 3,\n",
       " ('garden', 1.0): 7,\n",
       " ('midnight', 1.0): 1,\n",
       " ('sun', 1.0): 6,\n",
       " ('beauti', 1.0): 50,\n",
       " ('canal', 1.0): 1,\n",
       " ('dasvidaniya', 1.0): 1,\n",
       " ('till', 1.0): 18,\n",
       " ('scout', 1.0): 1,\n",
       " ('sg', 1.0): 1,\n",
       " ('futur', 1.0): 13,\n",
       " ('wlan', 1.0): 1,\n",
       " ('pro', 1.0): 5,\n",
       " ('confer', 1.0): 1,\n",
       " ('asia', 1.0): 1,\n",
       " ('chang', 1.0): 24,\n",
       " ('lollipop', 1.0): 1,\n",
       " ('🍭', 1.0): 1,\n",
       " ('nez', 1.0): 1,\n",
       " ('agnezmo', 1.0): 1,\n",
       " ('oley', 1.0): 1,\n",
       " ('mama', 1.0): 1,\n",
       " ('stand', 1.0): 8,\n",
       " ('stronger', 1.0): 1,\n",
       " ('god', 1.0): 20,\n",
       " ('misti', 1.0): 1,\n",
       " ('babi', 1.0): 20,\n",
       " ('cute', 1.0): 26,\n",
       " ('woohoo', 1.0): 3,\n",
       " (\"can't\", 1.0): 43,\n",
       " ('sign', 1.0): 11,\n",
       " ('yet', 1.0): 13,\n",
       " ('still', 1.0): 48,\n",
       " ('think', 1.0): 63,\n",
       " ('mka', 1.0): 5,\n",
       " ('liam', 1.0): 8,\n",
       " ('access', 1.0): 3,\n",
       " ('welcom', 1.0): 73,\n",
       " ('stat', 1.0): 60,\n",
       " ('arriv', 1.0): 67,\n",
       " ('1', 1.0): 75,\n",
       " ('unfollow', 1.0): 63,\n",
       " ('via', 1.0): 69,\n",
       " ('surpris', 1.0): 10,\n",
       " ('figur', 1.0): 5,\n",
       " ('happybirthdayemilybett', 1.0): 1,\n",
       " ('sweet', 1.0): 19,\n",
       " ('talent', 1.0): 5,\n",
       " ('2', 1.0): 58,\n",
       " ('plan', 1.0): 27,\n",
       " ('drain', 1.0): 1,\n",
       " ('gotta', 1.0): 5,\n",
       " ('timezon', 1.0): 1,\n",
       " ('parent', 1.0): 5,\n",
       " ('proud', 1.0): 12,\n",
       " ('least', 1.0): 16,\n",
       " ('mayb', 1.0): 18,\n",
       " ('sometim', 1.0): 13,\n",
       " ('grade', 1.0): 4,\n",
       " ('al', 1.0): 4,\n",
       " ('grand', 1.0): 4,\n",
       " ('manila_bro', 1.0): 2,\n",
       " ('chosen', 1.0): 1,\n",
       " ('let', 1.0): 68,\n",
       " ('around', 1.0): 17,\n",
       " ('..', 1.0): 128,\n",
       " ('side', 1.0): 15,\n",
       " ('world', 1.0): 27,\n",
       " ('eh', 1.0): 2,\n",
       " ('take', 1.0): 43,\n",
       " ('care', 1.0): 18,\n",
       " ('final', 1.0): 30,\n",
       " ('fuck', 1.0): 26,\n",
       " ('weekend', 1.0): 75,\n",
       " ('real', 1.0): 21,\n",
       " ('x45', 1.0): 1,\n",
       " ('join', 1.0): 23,\n",
       " ('hushedcallwithfraydo', 1.0): 1,\n",
       " ('gift', 1.0): 8,\n",
       " ('yeahhh', 1.0): 1,\n",
       " ('hushedpinwithsammi', 1.0): 2,\n",
       " ('event', 1.0): 8,\n",
       " ('might', 1.0): 27,\n",
       " ('luv', 1.0): 6,\n",
       " ('realli', 1.0): 79,\n",
       " ('appreci', 1.0): 31,\n",
       " ('share', 1.0): 46,\n",
       " ('wow', 1.0): 22,\n",
       " ('tom', 1.0): 5,\n",
       " ('gym', 1.0): 4,\n",
       " ('monday', 1.0): 9,\n",
       " ('invit', 1.0): 17,\n",
       " ('scope', 1.0): 5,\n",
       " ('friend', 1.0): 61,\n",
       " ('nude', 1.0): 2,\n",
       " ('sleep', 1.0): 45,\n",
       " ('birthday', 1.0): 74,\n",
       " ('want', 1.0): 96,\n",
       " ('t-shirt', 1.0): 3,\n",
       " ('cool', 1.0): 38,\n",
       " ('haw', 1.0): 1,\n",
       " ('phela', 1.0): 1,\n",
       " ('mom', 1.0): 10,\n",
       " ('obvious', 1.0): 2,\n",
       " ('princ', 1.0): 1,\n",
       " ('charm', 1.0): 1,\n",
       " ('stage', 1.0): 2,\n",
       " ('luck', 1.0): 30,\n",
       " ('tyler', 1.0): 2,\n",
       " ('hipster', 1.0): 1,\n",
       " ('glass', 1.0): 5,\n",
       " ('marti', 1.0): 2,\n",
       " ('glad', 1.0): 43,\n",
       " ('done', 1.0): 54,\n",
       " ('afternoon', 1.0): 10,\n",
       " ('read', 1.0): 34,\n",
       " ('kahfi', 1.0): 1,\n",
       " ('finish', 1.0): 17,\n",
       " ('ohmyg', 1.0): 1,\n",
       " ('yaya', 1.0): 3,\n",
       " ('dub', 1.0): 2,\n",
       " ('stalk', 1.0): 2,\n",
       " ('ig', 1.0): 3,\n",
       " ('gondooo', 1.0): 1,\n",
       " ('moo', 1.0): 2,\n",
       " ('tologooo', 1.0): 1,\n",
       " ('becom', 1.0): 10,\n",
       " ('detail', 1.0): 10,\n",
       " ('zzz', 1.0): 1,\n",
       " ('xx', 1.0): 42,\n",
       " ('physiotherapi', 1.0): 1,\n",
       " ('hashtag', 1.0): 5,\n",
       " ('💪', 1.0): 1,\n",
       " ('monica', 1.0): 1,\n",
       " ('miss', 1.0): 27,\n",
       " ('sound', 1.0): 23,\n",
       " ('morn', 1.0): 101,\n",
       " (\"that'\", 1.0): 67,\n",
       " ('x43', 1.0): 1,\n",
       " ('definit', 1.0): 23,\n",
       " ('tri', 1.0): 44,\n",
       " ('tonight', 1.0): 20,\n",
       " ('took', 1.0): 8,\n",
       " ('advic', 1.0): 6,\n",
       " ('treviso', 1.0): 1,\n",
       " ('concert', 1.0): 24,\n",
       " ('citi', 1.0): 27,\n",
       " ('countri', 1.0): 23,\n",
       " (\"i'll\", 1.0): 90,\n",
       " ('start', 1.0): 61,\n",
       " ('fine', 1.0): 10,\n",
       " ('gorgeou', 1.0): 12,\n",
       " ('xo', 1.0): 2,\n",
       " ('oven', 1.0): 3,\n",
       " ('roast', 1.0): 2,\n",
       " ('garlic', 1.0): 1,\n",
       " ('oliv', 1.0): 1,\n",
       " ('oil', 1.0): 4,\n",
       " ('dri', 1.0): 5,\n",
       " ('tomato', 1.0): 1,\n",
       " ('basil', 1.0): 1,\n",
       " ('centuri', 1.0): 1,\n",
       " ('tuna', 1.0): 1,\n",
       " ('right', 1.0): 47,\n",
       " ('back', 1.0): 98,\n",
       " ('atchya', 1.0): 1,\n",
       " ('even', 1.0): 35,\n",
       " ('almost', 1.0): 10,\n",
       " ('chanc', 1.0): 6,\n",
       " ('cheer', 1.0): 20,\n",
       " ('po', 1.0): 4,\n",
       " ('ice', 1.0): 6,\n",
       " ('cream', 1.0): 6,\n",
       " ('agre', 1.0): 16,\n",
       " ('100', 1.0): 8,\n",
       " ('heheheh', 1.0): 2,\n",
       " ('that', 1.0): 13,\n",
       " ('point', 1.0): 13,\n",
       " ('stay', 1.0): 25,\n",
       " ('home', 1.0): 31,\n",
       " ('soon', 1.0): 47,\n",
       " ('promis', 1.0): 6,\n",
       " ('web', 1.0): 4,\n",
       " ('whatsapp', 1.0): 5,\n",
       " ('volta', 1.0): 1,\n",
       " ('funcionar', 1.0): 1,\n",
       " ('com', 1.0): 2,\n",
       " ('iphon', 1.0): 7,\n",
       " ('jailbroken', 1.0): 1,\n",
       " ('later', 1.0): 16,\n",
       " ('34', 1.0): 3,\n",
       " ('min', 1.0): 9,\n",
       " ('leia', 1.0): 1,\n",
       " ('appear', 1.0): 3,\n",
       " ('hologram', 1.0): 1,\n",
       " ('r2d2', 1.0): 1,\n",
       " ('w', 1.0): 18,\n",
       " ('messag', 1.0): 10,\n",
       " ('obi', 1.0): 1,\n",
       " ('wan', 1.0): 3,\n",
       " ('sit', 1.0): 8,\n",
       " ('luke', 1.0): 6,\n",
       " ('inter', 1.0): 1,\n",
       " ('3', 1.0): 31,\n",
       " ('ucl', 1.0): 1,\n",
       " ('arsen', 1.0): 2,\n",
       " ('small', 1.0): 4,\n",
       " ('team', 1.0): 29,\n",
       " ('pass', 1.0): 12,\n",
       " ('🚂', 1.0): 1,\n",
       " ('dewsburi', 1.0): 2,\n",
       " ('railway', 1.0): 1,\n",
       " ('station', 1.0): 4,\n",
       " ('dew', 1.0): 1,\n",
       " ('west', 1.0): 3,\n",
       " ('yorkshir', 1.0): 2,\n",
       " ('430', 1.0): 1,\n",
       " ('smh', 1.0): 2,\n",
       " ('9:25', 1.0): 1,\n",
       " ('live', 1.0): 26,\n",
       " ('strang', 1.0): 4,\n",
       " ('imagin', 1.0): 5,\n",
       " ('megan', 1.0): 1,\n",
       " ('masaantoday', 1.0): 6,\n",
       " ('a4', 1.0): 3,\n",
       " ('shweta', 1.0): 1,\n",
       " ('tripathi', 1.0): 1,\n",
       " ('5', 1.0): 18,\n",
       " ('20', 1.0): 6,\n",
       " ('kurta', 1.0): 3,\n",
       " ('half', 1.0): 7,\n",
       " ('number', 1.0): 13,\n",
       " ('wsalelov', 1.0): 16,\n",
       " ('ah', 1.0): 13,\n",
       " ('larri', 1.0): 3,\n",
       " ('anyway', 1.0): 16,\n",
       " ('kinda', 1.0): 13,\n",
       " ('goood', 1.0): 4,\n",
       " ('life', 1.0): 49,\n",
       " ('enn', 1.0): 1,\n",
       " ('could', 1.0): 32,\n",
       " ('warmup', 1.0): 1,\n",
       " ('15th', 1.0): 2,\n",
       " ('bath', 1.0): 7,\n",
       " ('dum', 1.0): 2,\n",
       " ('andar', 1.0): 1,\n",
       " ('ram', 1.0): 1,\n",
       " ('sampath', 1.0): 1,\n",
       " ('sona', 1.0): 1,\n",
       " ('mohapatra', 1.0): 1,\n",
       " ('samantha', 1.0): 1,\n",
       " ('edward', 1.0): 1,\n",
       " ('mein', 1.0): 1,\n",
       " ('tulan', 1.0): 1,\n",
       " ('razi', 1.0): 2,\n",
       " ('wah', 1.0): 2,\n",
       " ('josh', 1.0): 1,\n",
       " ('alway', 1.0): 67,\n",
       " ('smile', 1.0): 62,\n",
       " ('pictur', 1.0): 12,\n",
       " ('16.20', 1.0): 1,\n",
       " ('giveitup', 1.0): 1,\n",
       " ('given', 1.0): 3,\n",
       " ('ga', 1.0): 3,\n",
       " ('subsidi', 1.0): 1,\n",
       " ('initi', 1.0): 4,\n",
       " ('propos', 1.0): 3,\n",
       " ('delight', 1.0): 7,\n",
       " ('yesterday', 1.0): 7,\n",
       " ('x42', 1.0): 1,\n",
       " ('lmaoo', 1.0): 2,\n",
       " ('song', 1.0): 22,\n",
       " ('ever', 1.0): 23,\n",
       " ('shall', 1.0): 6,\n",
       " ('littl', 1.0): 31,\n",
       " ('throwback', 1.0): 3,\n",
       " ('outli', 1.0): 1,\n",
       " ('island', 1.0): 5,\n",
       " ('cheung', 1.0): 1,\n",
       " ('chau', 1.0): 1,\n",
       " ('mui', 1.0): 1,\n",
       " ('wo', 1.0): 1,\n",
       " ('total', 1.0): 9,\n",
       " ('differ', 1.0): 11,\n",
       " ('kfckitchentour', 1.0): 2,\n",
       " ('kitchen', 1.0): 4,\n",
       " ('clean', 1.0): 1,\n",
       " (\"i'm\", 1.0): 183,\n",
       " ('cusp', 1.0): 1,\n",
       " ('test', 1.0): 7,\n",
       " ('water', 1.0): 8,\n",
       " ('reward', 1.0): 1,\n",
       " ('arummzz', 1.0): 2,\n",
       " (\"let'\", 1.0): 23,\n",
       " ('drive', 1.0): 11,\n",
       " ('travel', 1.0): 20,\n",
       " ('yogyakarta', 1.0): 3,\n",
       " ('jeep', 1.0): 3,\n",
       " ('indonesia', 1.0): 4,\n",
       " ('instamood', 1.0): 3,\n",
       " ('wanna', 1.0): 30,\n",
       " ('skype', 1.0): 3,\n",
       " ('may', 1.0): 22,\n",
       " ('nice', 1.0): 98,\n",
       " ('friendli', 1.0): 2,\n",
       " ('pretend', 1.0): 2,\n",
       " ('film', 1.0): 9,\n",
       " ('congratul', 1.0): 15,\n",
       " ('winner', 1.0): 4,\n",
       " ('cheesydelight', 1.0): 1,\n",
       " ('contest', 1.0): 6,\n",
       " ('address', 1.0): 10,\n",
       " ('guy', 1.0): 60,\n",
       " ('market', 1.0): 5,\n",
       " ('24/7', 1.0): 1,\n",
       " ('14', 1.0): 1,\n",
       " ('hour', 1.0): 27,\n",
       " ('leav', 1.0): 12,\n",
       " ('without', 1.0): 12,\n",
       " ('delay', 1.0): 2,\n",
       " ('actual', 1.0): 19,\n",
       " ('easi', 1.0): 9,\n",
       " ('guess', 1.0): 14,\n",
       " ('train', 1.0): 10,\n",
       " ('wd', 1.0): 1,\n",
       " ('shift', 1.0): 5,\n",
       " ('engin', 1.0): 2,\n",
       " ('etc', 1.0): 2,\n",
       " ('sunburn', 1.0): 1,\n",
       " ('peel', 1.0): 2,\n",
       " ('blog', 1.0): 31,\n",
       " ('huge', 1.0): 11,\n",
       " ('warm', 1.0): 6,\n",
       " ('☆', 1.0): 3,\n",
       " ('complet', 1.0): 11,\n",
       " ('triangl', 1.0): 2,\n",
       " ('northern', 1.0): 1,\n",
       " ('ireland', 1.0): 2,\n",
       " ('sight', 1.0): 1,\n",
       " ('smthng', 1.0): 2,\n",
       " ('fr', 1.0): 3,\n",
       " ('hug', 1.0): 13,\n",
       " ('xoxo', 1.0): 3,\n",
       " ('uu', 1.0): 1,\n",
       " ('jaann', 1.0): 1,\n",
       " ('topnewfollow', 1.0): 2,\n",
       " ('connect', 1.0): 14,\n",
       " ('wonder', 1.0): 35,\n",
       " ('made', 1.0): 53,\n",
       " ('fluffi', 1.0): 1,\n",
       " ('insid', 1.0): 8,\n",
       " ('pirouett', 1.0): 1,\n",
       " ('moos', 1.0): 1,\n",
       " ('trip', 1.0): 14,\n",
       " ('philli', 1.0): 1,\n",
       " ('decemb', 1.0): 3,\n",
       " (\"i'd\", 1.0): 20,\n",
       " ('dude', 1.0): 6,\n",
       " ('x41', 1.0): 1,\n",
       " ('question', 1.0): 17,\n",
       " ('flaw', 1.0): 1,\n",
       " ('pain', 1.0): 9,\n",
       " ('negat', 1.0): 1,\n",
       " ('strength', 1.0): 3,\n",
       " ('went', 1.0): 12,\n",
       " ('solo', 1.0): 4,\n",
       " ('move', 1.0): 12,\n",
       " ('fav', 1.0): 13,\n",
       " ('nirvana', 1.0): 1,\n",
       " ('smell', 1.0): 2,\n",
       " ('teen', 1.0): 3,\n",
       " ('spirit', 1.0): 3,\n",
       " ('rip', 1.0): 3,\n",
       " ('ami', 1.0): 4,\n",
       " ('winehous', 1.0): 1,\n",
       " ('coupl', 1.0): 9,\n",
       " ('tomhiddleston', 1.0): 1,\n",
       " ('elizabetholsen', 1.0): 1,\n",
       " ('yaytheylookgreat', 1.0): 1,\n",
       " ('goodnight', 1.0): 24,\n",
       " ('vid', 1.0): 11,\n",
       " ('wake', 1.0): 12,\n",
       " ('gonna', 1.0): 21,\n",
       " ('shoot', 1.0): 6,\n",
       " ('itti', 1.0): 2,\n",
       " ('bitti', 1.0): 2,\n",
       " ('teeni', 1.0): 2,\n",
       " ('bikini', 1.0): 3,\n",
       " ('much', 1.0): 89,\n",
       " ('4th', 1.0): 4,\n",
       " ('togeth', 1.0): 7,\n",
       " ('end', 1.0): 20,\n",
       " ('xfile', 1.0): 1,\n",
       " ('content', 1.0): 4,\n",
       " ('rain', 1.0): 21,\n",
       " ('fabul', 1.0): 5,\n",
       " ('fantast', 1.0): 13,\n",
       " ('♡', 1.0): 20,\n",
       " ('jb', 1.0): 1,\n",
       " ('forev', 1.0): 5,\n",
       " ('belieb', 1.0): 3,\n",
       " ('nighti', 1.0): 1,\n",
       " ('bug', 1.0): 3,\n",
       " ('bite', 1.0): 1,\n",
       " ('bracelet', 1.0): 2,\n",
       " ('idea', 1.0): 26,\n",
       " ('foundri', 1.0): 1,\n",
       " ('game', 1.0): 27,\n",
       " ('sens', 1.0): 7,\n",
       " ('pic', 1.0): 27,\n",
       " ('ef', 1.0): 1,\n",
       " ('phone', 1.0): 19,\n",
       " ('woot', 1.0): 2,\n",
       " ('derek', 1.0): 1,\n",
       " ('use', 1.0): 44,\n",
       " ('parkshar', 1.0): 1,\n",
       " ('gloucestershir', 1.0): 1,\n",
       " ('aaaahhh', 1.0): 1,\n",
       " ('man', 1.0): 23,\n",
       " ('traffic', 1.0): 2,\n",
       " ('stress', 1.0): 8,\n",
       " ('reliev', 1.0): 1,\n",
       " (\"how'r\", 1.0): 1,\n",
       " ('arbeloa', 1.0): 1,\n",
       " ('turn', 1.0): 15,\n",
       " ('17', 1.0): 4,\n",
       " ('omg', 1.0): 15,\n",
       " ('say', 1.0): 61,\n",
       " ('europ', 1.0): 1,\n",
       " ('rise', 1.0): 2,\n",
       " ('find', 1.0): 23,\n",
       " ('hard', 1.0): 12,\n",
       " ('believ', 1.0): 9,\n",
       " ('uncount', 1.0): 1,\n",
       " ('coz', 1.0): 3,\n",
       " ('unlimit', 1.0): 1,\n",
       " ('cours', 1.0): 18,\n",
       " ('teamposit', 1.0): 1,\n",
       " ('aldub', 1.0): 2,\n",
       " ('☕', 1.0): 3,\n",
       " ('rita', 1.0): 2,\n",
       " ('info', 1.0): 13,\n",
       " (\"we'd\", 1.0): 4,\n",
       " ('way', 1.0): 46,\n",
       " ('boy', 1.0): 21,\n",
       " ('x40', 1.0): 1,\n",
       " ('true', 1.0): 22,\n",
       " ('sethi', 1.0): 2,\n",
       " ('high', 1.0): 7,\n",
       " ('exe', 1.0): 1,\n",
       " ('skeem', 1.0): 1,\n",
       " ('saam', 1.0): 1,\n",
       " ('peopl', 1.0): 48,\n",
       " ('polit', 1.0): 2,\n",
       " ('izzat', 1.0): 1,\n",
       " ('wese', 1.0): 1,\n",
       " ('trust', 1.0): 9,\n",
       " ('khawateen', 1.0): 1,\n",
       " ('k', 1.0): 9,\n",
       " ('sath', 1.0): 2,\n",
       " ('mana', 1.0): 1,\n",
       " ('kar', 1.0): 1,\n",
       " ('deya', 1.0): 1,\n",
       " ('sort', 1.0): 9,\n",
       " ('smart', 1.0): 5,\n",
       " ('hair', 1.0): 12,\n",
       " ('tbh', 1.0): 5,\n",
       " ('jacob', 1.0): 2,\n",
       " ('g', 1.0): 10,\n",
       " ('upgrad', 1.0): 6,\n",
       " ('tee', 1.0): 2,\n",
       " ('famili', 1.0): 19,\n",
       " ('person', 1.0): 19,\n",
       " ('two', 1.0): 22,\n",
       " ('convers', 1.0): 6,\n",
       " ('onlin', 1.0): 7,\n",
       " ('mclaren', 1.0): 1,\n",
       " ('fridayfeel', 1.0): 5,\n",
       " ('tgif', 1.0): 10,\n",
       " ('squar', 1.0): 1,\n",
       " ('enix', 1.0): 1,\n",
       " ('bissmillah', 1.0): 1,\n",
       " ('ya', 1.0): 23,\n",
       " ('allah', 1.0): 3,\n",
       " (\"we'r\", 1.0): 29,\n",
       " ('socent', 1.0): 1,\n",
       " ('startup', 1.0): 2,\n",
       " ('drop', 1.0): 9,\n",
       " ('your', 1.0): 3,\n",
       " ('arnd', 1.0): 1,\n",
       " ('town', 1.0): 5,\n",
       " ('basic', 1.0): 4,\n",
       " ('piss', 1.0): 3,\n",
       " ('cup', 1.0): 4,\n",
       " ('also', 1.0): 35,\n",
       " ('terribl', 1.0): 2,\n",
       " ('complic', 1.0): 1,\n",
       " ('discuss', 1.0): 3,\n",
       " ('snapchat', 1.0): 36,\n",
       " ('lynettelow', 1.0): 1,\n",
       " ('kikmenow', 1.0): 3,\n",
       " ('snapm', 1.0): 2,\n",
       " ('hot', 1.0): 24,\n",
       " ('amazon', 1.0): 1,\n",
       " ('kikmeguy', 1.0): 3,\n",
       " ('defin', 1.0): 2,\n",
       " ('grow', 1.0): 7,\n",
       " ('sport', 1.0): 4,\n",
       " ('rt', 1.0): 12,\n",
       " ('rakyat', 1.0): 1,\n",
       " ('write', 1.0): 13,\n",
       " ('sinc', 1.0): 15,\n",
       " ('mention', 1.0): 24,\n",
       " ('fli', 1.0): 5,\n",
       " ('fish', 1.0): 3,\n",
       " ('promot', 1.0): 5,\n",
       " ('post', 1.0): 21,\n",
       " ('cyber', 1.0): 1,\n",
       " ('ourdaughtersourprid', 1.0): 5,\n",
       " ('mypapamyprid', 1.0): 2,\n",
       " ('papa', 1.0): 2,\n",
       " ('coach', 1.0): 2,\n",
       " ('posit', 1.0): 8,\n",
       " ('kha', 1.0): 1,\n",
       " ('atleast', 1.0): 2,\n",
       " ('x39', 1.0): 1,\n",
       " ('mango', 1.0): 1,\n",
       " (\"lassi'\", 1.0): 1,\n",
       " (\"monty'\", 1.0): 1,\n",
       " ('marvel', 1.0): 2,\n",
       " ('though', 1.0): 19,\n",
       " ('suspect', 1.0): 3,\n",
       " ('meant', 1.0): 3,\n",
       " ('24', 1.0): 4,\n",
       " ('hr', 1.0): 2,\n",
       " ('touch', 1.0): 15,\n",
       " ('kepler', 1.0): 4,\n",
       " ('452b', 1.0): 5,\n",
       " ('chalna', 1.0): 1,\n",
       " ('hai', 1.0): 11,\n",
       " ('thankyou', 1.0): 14,\n",
       " ('hazel', 1.0): 1,\n",
       " ('food', 1.0): 6,\n",
       " ('brooklyn', 1.0): 1,\n",
       " ('pta', 1.0): 2,\n",
       " ('awak', 1.0): 10,\n",
       " ('okayi', 1.0): 2,\n",
       " ('awww', 1.0): 15,\n",
       " ('ha', 1.0): 23,\n",
       " ('doc', 1.0): 1,\n",
       " ('splendid', 1.0): 1,\n",
       " ('spam', 1.0): 1,\n",
       " ('folder', 1.0): 1,\n",
       " ('amount', 1.0): 1,\n",
       " ('nigeria', 1.0): 1,\n",
       " ('claim', 1.0): 1,\n",
       " ('rted', 1.0): 1,\n",
       " ('leg', 1.0): 5,\n",
       " ('hurt', 1.0): 8,\n",
       " ('bad', 1.0): 18,\n",
       " ('mine', 1.0): 14,\n",
       " ('saturday', 1.0): 8,\n",
       " ('thaaank', 1.0): 1,\n",
       " ('puhon', 1.0): 1,\n",
       " ('happinesss', 1.0): 1,\n",
       " ('tnc', 1.0): 1,\n",
       " ('prior', 1.0): 1,\n",
       " ('notif', 1.0): 2,\n",
       " ('fat', 1.0): 1,\n",
       " ('co', 1.0): 1,\n",
       " ('probabl', 1.0): 9,\n",
       " ('ate', 1.0): 4,\n",
       " ('yuna', 1.0): 2,\n",
       " ('tamesid', 1.0): 1,\n",
       " ('´', 1.0): 3,\n",
       " ('googl', 1.0): 6,\n",
       " ('account', 1.0): 19,\n",
       " ('scouser', 1.0): 1,\n",
       " ('everyth', 1.0): 13,\n",
       " ('zoe', 1.0): 2,\n",
       " ('mate', 1.0): 7,\n",
       " ('liter', 1.0): 6,\n",
       " (\"they'r\", 1.0): 12,\n",
       " ('samee', 1.0): 1,\n",
       " ('edgar', 1.0): 1,\n",
       " ('updat', 1.0): 13,\n",
       " ('log', 1.0): 4,\n",
       " ('bring', 1.0): 17,\n",
       " ('abe', 1.0): 1,\n",
       " ('meet', 1.0): 34,\n",
       " ('x38', 1.0): 1,\n",
       " ('sigh', 1.0): 3,\n",
       " ('dreamili', 1.0): 1,\n",
       " ('pout', 1.0): 1,\n",
       " ('eye', 1.0): 14,\n",
       " ('quacketyquack', 1.0): 7,\n",
       " ('funni', 1.0): 19,\n",
       " ('happen', 1.0): 16,\n",
       " ('phil', 1.0): 1,\n",
       " ('em', 1.0): 3,\n",
       " ('del', 1.0): 1,\n",
       " ('rodder', 1.0): 1,\n",
       " ('els', 1.0): 10,\n",
       " ('play', 1.0): 46,\n",
       " ('newest', 1.0): 1,\n",
       " ('gamejam', 1.0): 1,\n",
       " ('irish', 1.0): 2,\n",
       " ('literatur', 1.0): 2,\n",
       " ('inaccess', 1.0): 2,\n",
       " (\"kareena'\", 1.0): 2,\n",
       " ('fan', 1.0): 30,\n",
       " ('brain', 1.0): 13,\n",
       " ('dot', 1.0): 11,\n",
       " ('braindot', 1.0): 11,\n",
       " ('fair', 1.0): 5,\n",
       " ('rush', 1.0): 1,\n",
       " ('either', 1.0): 11,\n",
       " ('brandi', 1.0): 1,\n",
       " ('18', 1.0): 5,\n",
       " ('carniv', 1.0): 1,\n",
       " ('men', 1.0): 10,\n",
       " ('put', 1.0): 17,\n",
       " ('mask', 1.0): 3,\n",
       " ('xavier', 1.0): 1,\n",
       " ('forneret', 1.0): 1,\n",
       " ('jennif', 1.0): 1,\n",
       " ('site', 1.0): 9,\n",
       " ('free', 1.0): 37,\n",
       " ('50.000', 1.0): 3,\n",
       " ('8', 1.0): 10,\n",
       " ('ball', 1.0): 7,\n",
       " ('pool', 1.0): 5,\n",
       " ('coin', 1.0): 5,\n",
       " ('edit', 1.0): 7,\n",
       " ('trish', 1.0): 1,\n",
       " ('♥', 1.0): 19,\n",
       " ('grate', 1.0): 5,\n",
       " ('three', 1.0): 10,\n",
       " ('comment', 1.0): 8,\n",
       " ('wakeup', 1.0): 1,\n",
       " ('besid', 1.0): 2,\n",
       " ('dirti', 1.0): 2,\n",
       " ('sex', 1.0): 6,\n",
       " ('lmaooo', 1.0): 1,\n",
       " ('😤', 1.0): 2,\n",
       " ('loui', 1.0): 4,\n",
       " (\"he'\", 1.0): 11,\n",
       " ('throw', 1.0): 3,\n",
       " ('caus', 1.0): 15,\n",
       " ('inspir', 1.0): 7,\n",
       " ('ff', 1.0): 48,\n",
       " ('twoof', 1.0): 3,\n",
       " ('gr8', 1.0): 1,\n",
       " ('wkend', 1.0): 3,\n",
       " ('kind', 1.0): 24,\n",
       " ('exhaust', 1.0): 2,\n",
       " ('word', 1.0): 20,\n",
       " ('cheltenham', 1.0): 1,\n",
       " ('area', 1.0): 4,\n",
       " ('9', 1.0): 4,\n",
       " ('kale', 1.0): 1,\n",
       " ('crisp', 1.0): 1,\n",
       " ('ruin', 1.0): 5,\n",
       " ('x37', 1.0): 1,\n",
       " ('open', 1.0): 12,\n",
       " ('worldwid', 1.0): 2,\n",
       " ('outta', 1.0): 1,\n",
       " ('sfvbeta', 1.0): 1,\n",
       " ('vantast', 1.0): 1,\n",
       " ('xcylin', 1.0): 1,\n",
       " ('bundl', 1.0): 1,\n",
       " ('show', 1.0): 28,\n",
       " ('internet', 1.0): 2,\n",
       " ('price', 1.0): 4,\n",
       " ('realisticli', 1.0): 1,\n",
       " ('pay', 1.0): 8,\n",
       " ('net', 1.0): 1,\n",
       " ('educ', 1.0): 1,\n",
       " ('power', 1.0): 7,\n",
       " ('weapon', 1.0): 1,\n",
       " ('nelson', 1.0): 1,\n",
       " ('mandela', 1.0): 1,\n",
       " ('recent', 1.0): 9,\n",
       " ('j', 1.0): 3,\n",
       " ('chenab', 1.0): 1,\n",
       " ('flow', 1.0): 5,\n",
       " ('pakistan', 1.0): 2,\n",
       " ('incredibleindia', 1.0): 1,\n",
       " ('teenchoic', 1.0): 10,\n",
       " ('choiceinternationalartist', 1.0): 9,\n",
       " ('superjunior', 1.0): 9,\n",
       " ('caught', 1.0): 4,\n",
       " ('first', 1.0): 50,\n",
       " ('salmon', 1.0): 3,\n",
       " ('super-blend', 1.0): 1,\n",
       " ('project', 1.0): 6,\n",
       " ('youth@bipolaruk.org.uk', 1.0): 1,\n",
       " ('awesom', 1.0): 42,\n",
       " ('stream', 1.0): 14,\n",
       " ('alma', 1.0): 1,\n",
       " ('mater', 1.0): 1,\n",
       " ('highschoolday', 1.0): 1,\n",
       " ('clientvisit', 1.0): 1,\n",
       " ('faith', 1.0): 3,\n",
       " ('christian', 1.0): 1,\n",
       " ('school', 1.0): 9,\n",
       " ('lizaminnelli', 1.0): 1,\n",
       " ('upcom', 1.0): 2,\n",
       " ('uk', 1.0): 4,\n",
       " ('😄', 1.0): 5,\n",
       " ('singl', 1.0): 6,\n",
       " ('hill', 1.0): 4,\n",
       " ('everi', 1.0): 26,\n",
       " ('beat', 1.0): 10,\n",
       " ('wrong', 1.0): 10,\n",
       " ('readi', 1.0): 25,\n",
       " ('natur', 1.0): 1,\n",
       " ('pefumeri', 1.0): 1,\n",
       " ('workshop', 1.0): 3,\n",
       " ('neal', 1.0): 1,\n",
       " ('yard', 1.0): 1,\n",
       " ('covent', 1.0): 1,\n",
       " ('tomorrow', 1.0): 40,\n",
       " ('fback', 1.0): 27,\n",
       " ...}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will create a vector of dimension 1x3 for each tweet<br>\n",
    "where 1st element will be bias<br>\n",
    "2nd element will be sum of frequencies of word in when label is positive<br>\n",
    "3rd element will be sum of frequencies of word in when label is negative<br>\n",
    "x(i) = [1, sum of frequencies of word in when label is positive, sum of frequencies of word in when label is negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['beauti',\n",
       " 'sunflow',\n",
       " 'sunni',\n",
       " 'friday',\n",
       " 'morn',\n",
       " ':)',\n",
       " 'sunflow',\n",
       " 'favourit',\n",
       " 'happi',\n",
       " 'friday',\n",
       " '…']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet = tweets[2277]\n",
    "processed = process_tweet(tweet)\n",
    "processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vectorizing this tweet\n",
    "x = np.zeros(3)\n",
    "x[0] = 1 # bias term\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.000e+00, 4.223e+03, 1.190e+02])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for word in processed:\n",
    "    if (word, 1.0) in freqs:\n",
    "        x[1] += freqs[(word, 1.0)]\n",
    "    if (word, 0.0) in freqs:\n",
    "        x[2] += freqs[(word, 0.0)]\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> in this manner we will vectorize all the tweets using vectorize function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(tweets, freqs):\n",
    "    processed = process_tweet(tweets)\n",
    "    x = np.zeros(3)\n",
    "    x[0] = 1\n",
    "    for word in processed:\n",
    "        if (word, 1.0) in freqs:\n",
    "            x[1] += freqs[(word, 1.0)]\n",
    "        if (word, 0.0) in freqs:\n",
    "            x[2] += freqs[(word, 0.0)]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> creating the traing features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = positive_tweets[:4000]\n",
    "train_neg = negative_tweets[:4000]\n",
    "train_x = train_pos + train_neg\n",
    "train_y =  np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "\n",
    "test_pos = positive_tweets[4000:]\n",
    "test_neg = negative_tweets[4000:]\n",
    "test_x = test_pos + test_neg\n",
    "test_y =  np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.000e+00, 3.764e+03, 7.200e+01],\n",
       "       [1.000e+00, 4.464e+03, 5.170e+02],\n",
       "       [1.000e+00, 3.759e+03, 1.600e+02],\n",
       "       ...,\n",
       "       [1.000e+00, 1.840e+02, 9.890e+02],\n",
       "       [1.000e+00, 2.560e+02, 4.855e+03],\n",
       "       [1.000e+00, 2.400e+02, 4.967e+03]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.zeros((len(train_x), 3))\n",
    "for i in range(len(train_x)):\n",
    "    X[i] = vectorize(train_x[i], freqs)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = train_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
